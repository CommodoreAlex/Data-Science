{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7290304a-e6dd-45a8-afd6-e18dbeb35c10",
   "metadata": {},
   "source": [
    "# Hack The Box Skills Assessment: Sentiment Analysis on IMDB Movie Reviews\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project serves as the skills assessment for the **Applications of AI in Infosec** module, part of the **Hack The Box AI Red Teamer Path**. The module is designed to equip learners with essential data science and machine learning skills, focusing on applications within cybersecurity.\n",
    "\n",
    "For this assessment, I am working with the **IMDB dataset** introduced by Maas et al. (2011). This dataset consists of 50,000 movie reviews extracted from the Internet Movie Database (IMDB), annotated for sentiment analysis. The reviews are balanced with an equal number of positive and negative examples and are divided into training and test sets. This curated mixture of reviews makes the dataset a valuable resource for evaluating various natural language processing (NLP) techniques and machine learning models for sentiment classification tasks.\n",
    "\n",
    "The objective of this project is to develop a model capable of predicting whether a given movie review is **positive (1)** or **negative (0)**. The IMDB dataset has become a cornerstone for research in NLP, particularly for developing word representations like word embeddings. By leveraging this dataset, I aim to benchmark and optimize machine learning models in the domain of sentiment classification.\n",
    "\n",
    "## Model Evaluation and Flag Submission\n",
    "\n",
    "After training and evaluating the model, I will upload it to the **Hack The Box Playground VM** evaluation portal for testing. If the model meets the specified performance criteria, a **flag value** will be generated. This flag serves as confirmation of the model's success and completion of the project.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca89b2ad-6e96-42ef-8234-7e363963917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow keras numpy pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756348c6-08ce-4308-8371-2a614a2e049f",
   "metadata": {},
   "source": [
    "# Download the Dataset and Unzip\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1dd4c6-a765-4ab3-bb33-b130266da98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Successful.\n"
     ]
    }
   ],
   "source": [
    "# Downloading the Dataset\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://academy.hackthebox.com/storage/modules/292/skills_assessment_data.zip\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Download Successful.\")\n",
    "else:\n",
    "    print(\"Failed to download the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ed62d8-8b7d-49a1-8763-ed42d3c14b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction Complete\n",
      "Extracted files:  ['test.json', 'train.json']\n"
     ]
    }
   ],
   "source": [
    "# Unzipping the dataset\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"skills_assessment_data\")\n",
    "    print(\"Extraction Complete\")\n",
    "\n",
    "# Verify complete extraction\n",
    "import os\n",
    "extracted_files = os.listdir(\"skills_assessment_data\")\n",
    "print(\"Extracted files: \", extracted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a5e3c-0f19-4d97-8c7e-1f0c57901b6c",
   "metadata": {},
   "source": [
    "# Load and Inspect the Dataset\n",
    "We are loading the training and testing data in UTF-8 format, for the ability to process the JSON source files. Then we can convert these to data frames so we can do initial exploration on the dataset. Where we will have 'text' being the movie review and 'label' pointing to '1' for positive and '0' for negative sentiment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57c6099-a058-4fdc-8b8f-135afc5beb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
      "1  Homelessness (or Houselessness as George Carli...      1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
      "3  This is easily the most underrated film inn th...      1\n",
      "4  This is not the typical Mel Brooks film. It wa...      1\n",
      "                                                text  label\n",
      "0  I went and saw this movie last night after bei...      1\n",
      "1  Actor turned director Bill Paxton follows up h...      1\n",
      "2  As a recreational golfer with some knowledge o...      1\n",
      "3  I saw this film in a sneak preview, and it is ...      1\n",
      "4  Bill Paxton has taken the true story of the 19...      1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "with open(\"skills_assessment_data/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load testing data\n",
    "with open(\"skills_assessment_data/test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Convert to TWO DataFrame's for the train and test sets\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Display some examples\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f4d01-54cd-4351-9c20-44574c6b9622",
   "metadata": {},
   "source": [
    "#### Checking for missing values that skew and reduce the quality of data\n",
    "We find no missing values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1534de6-6f43-4940-8c2c-cdbad3b87364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missingvalues:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Missingvalues:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missingvalues:\\n\", train_df.isnull().sum())\n",
    "print(\"Missingvalues:\\n\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d2201-179b-45a3-ac39-eeb4f4ce8d56",
   "metadata": {},
   "source": [
    "#### Checking for duplicate values for the same purpose\n",
    "We do find a lot of duplicate values-  we can drop these,\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9924477-c1ed-40bc-9200-2d4c0ee6a15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries:\n",
      " 96\n",
      "Duplicate entries:\n",
      " 199\n"
     ]
    }
   ],
   "source": [
    "print(\"Duplicate entries:\\n\", train_df.duplicated().sum())\n",
    "print(\"Duplicate entries:\\n\", test_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afefd01-0602-424a-8c30-ba3943bfeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removign duplicate entries from both training and test datasets\n",
    "train_df = train_df.drop_duplicates()\n",
    "test_df = test_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "405f48c9-5289-49fe-b5a9-d049e90c2e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries:\n",
      " 0\n",
      "Duplicate entries:\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# Confirmation of deletion\n",
    "print(\"Duplicate entries:\\n\", train_df.duplicated().sum())\n",
    "print(\"Duplicate entries:\\n\", test_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b3779-2da0-46db-bb69-5a654b8b10b4",
   "metadata": {},
   "source": [
    "# Preprocess the Text Data\n",
    "This is going to standardize the text, reduce noise, extract meaningful features (all improve performance), relying on the NLTK library for tokenization, stop word removal, and stemming to better implement the Bayes Spam Classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "705445d6-cdad-4fc6-b43c-a0ec1a43c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ANY PREPROCESSINGM ===\n",
      "                                                text  label\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
      "1  Homelessness (or Houselessness as George Carli...      1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
      "3  This is easily the most underrated film inn th...      1\n",
      "4  This is not the typical Mel Brooks film. It wa...      1\n",
      "                                                text  label\n",
      "0  I went and saw this movie last night after bei...      1\n",
      "1  Actor turned director Bill Paxton follows up h...      1\n",
      "2  As a recreational golfer with some knowledge o...      1\n",
      "3  I saw this film in a sneak preview, and it is ...      1\n",
      "4  Bill Paxton has taken the true story of the 19...      1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "#nltk.download(\"punkt\") # tokenization\n",
    "#nltk.download(\"punkt_tab\")\n",
    "#nltk.download(\"stopwords\") # Stop words\n",
    "\n",
    "print(\"=== BEFORE ANY PREPROCESSINGM ===\")\n",
    "print(train_df.head(5))\n",
    "print(test_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42d473-e4a3-48bb-b555-8cb3dbe7ea22",
   "metadata": {},
   "source": [
    "#### Text Cleaning\n",
    "We want to remove any uncessary elements from the reviews like HTML Tags, special characters, extra whitespace. Then convert all text to lowercase to ensure uniformity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b472d2a4-faa7-4d86-825e-1ab6f9269cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING NON-NECESSARY ELEMENTS AND LOWERCASING TEXT ===\n",
      "0    bromwel high cartoon comedi ran time program s...\n",
      "1    homeless houseless georg carlin state issu yea...\n",
      "2    brilliant overact lesley ann warren best drama...\n",
      "3    easili underr film inn brook cannon sure flaw ...\n",
      "4    typic mel brook film much less slapstick movi ...\n",
      "Name: text, dtype: object\n",
      "0    went saw movi last night coax friend mine ill ...\n",
      "1    actor turn director bill paxton follow promis ...\n",
      "2    recreat golfer knowledg sport histori pleas di...\n",
      "3    saw film sneak preview delight cinematographi ...\n",
      "4    bill paxton taken true stori us golf open made...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing only to non-null entries\n",
    "train_df['text'] = train_df['text'].dropna().apply(clean_text)\n",
    "test_df['text'] = test_df['text'].dropna().apply(clean_text)\n",
    "\n",
    "print(\"\\n=== AFTER REMOVING NON-NECESSARY ELEMENTS AND LOWERCASING TEXT ===\")\n",
    "print(train_df[\"text\"].head(5))\n",
    "print(test_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf979dc-6609-484e-8700-0063ab69aa67",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "Break down each review into individual words or tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b26bc4b0-c19b-47d2-98f1-e262b172e85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER TOKENIZATION ===\n",
      "0    [bromwell, high, is, a, cartoon, comedy, it, r...\n",
      "1    [homelessness, or, houselessness, as, george, ...\n",
      "2    [brilliant, overacting, by, lesley, ann, warre...\n",
      "3    [this, is, easily, the, most, underrated, film...\n",
      "4    [this, is, not, the, typical, mel, brooks, fil...\n",
      "Name: text, dtype: object\n",
      "0    [i, went, and, saw, this, movie, last, night, ...\n",
      "1    [actor, turned, director, bill, paxton, follow...\n",
      "2    [as, a, recreational, golfer, with, some, know...\n",
      "3    [i, saw, this, film, in, a, sneak, preview, an...\n",
      "4    [bill, paxton, has, taken, the, true, story, o...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split each message into individual tokens\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(word_tokenize)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(word_tokenize)\n",
    "\n",
    "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
    "print(train_df[\"text\"].head(5))\n",
    "print(test_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57760116-8e31-49f3-9ce1-fa8bd0ee8882",
   "metadata": {},
   "source": [
    "#### Stop Word Removal\n",
    "Eliminate common words like \"is,\" \"the,\" \"and\" that donâ€™t contribute much meaning.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69fef2b4-ddab-4ce4-9b19-3fb507289766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING STOP WORDS ===\n",
      "0    [bromwell, high, cartoon, comedy, ran, time, p...\n",
      "1    [homelessness, houselessness, george, carlin, ...\n",
      "2    [brilliant, overacting, lesley, ann, warren, b...\n",
      "3    [easily, underrated, film, inn, brooks, cannon...\n",
      "4    [typical, mel, brooks, film, much, less, slaps...\n",
      "Name: text, dtype: object\n",
      "0    [went, saw, movie, last, night, coaxed, friend...\n",
      "1    [actor, turned, director, bill, paxton, follow...\n",
      "2    [recreational, golfer, knowledge, sports, hist...\n",
      "3    [saw, film, sneak, preview, delightful, cinema...\n",
      "4    [bill, paxton, taken, true, story, us, golf, o...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a set of English stop words and remove them from the tokens\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
    "print(train_df[\"text\"].head(5))\n",
    "print(test_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0d681-c62f-4941-bcd5-0a020433dad0",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Reduce words to their base or root form for consistency. For example, \"running\" becomes \"run.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a8ff6d9-126b-454a-854e-24a713f7ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER STEMMING ===\n",
      "0    [bromwel, high, cartoon, comedi, ran, time, pr...\n",
      "1    [homeless, houseless, georg, carlin, state, is...\n",
      "2    [brilliant, overact, lesley, ann, warren, best...\n",
      "3    [easili, underr, film, inn, brook, cannon, sur...\n",
      "4    [typic, mel, brook, film, much, less, slapstic...\n",
      "Name: text, dtype: object\n",
      "0    [went, saw, movi, last, night, coax, friend, m...\n",
      "1    [actor, turn, director, bill, paxton, follow, ...\n",
      "2    [recreat, golfer, knowledg, sport, histori, pl...\n",
      "3    [saw, film, sneak, preview, delight, cinematog...\n",
      "4    [bill, paxton, taken, true, stori, us, golf, o...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Stem each token to reduce words to their base form\n",
    "stemmer = PorterStemmer()\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "print(\"\\n=== AFTER STEMMING ===\")\n",
    "print(train_df[\"text\"].head(5))\n",
    "print(test_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984b48e-f60d-4cd6-9a34-d51bd9748351",
   "metadata": {},
   "source": [
    "#### Joining Tokens Back into a Single String\n",
    "Machine learning algorithms and vectorization techniques (e.g. TF_IDF) work better with raw strings. Rejoining tokens into a space-separated string restores a format compatible with these methods, preparing the dataset for the feature extraction phase.\n",
    "                                                                                                                                                                                                                          \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60342b84-3c01-4296-ae15-e7ceecc41a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
      "0    bromwel high cartoon comedi ran time program s...\n",
      "1    homeless houseless georg carlin state issu yea...\n",
      "2    brilliant overact lesley ann warren best drama...\n",
      "3    easili underr film inn brook cannon sure flaw ...\n",
      "4    typic mel brook film much less slapstick movi ...\n",
      "Name: text, dtype: object\n",
      "0    went saw movi last night coax friend mine ill ...\n",
      "1    actor turn director bill paxton follow promis ...\n",
      "2    recreat golfer knowledg sport histori pleas di...\n",
      "3    saw film sneak preview delight cinematographi ...\n",
      "4    bill paxton taken true stori us golf open made...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rejoin tokens into a single string for feature extraction\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: \" \".join(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
    "print(train_df[\"text\"].head(5))\n",
    "print(test_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1093562-fac0-4cef-abf6-20640d6b1a72",
   "metadata": {},
   "source": [
    "#### Bag-of-Words (BoW) Feature Extraction (Convert Text to Numerical Data)\n",
    "The purpose of this is to transform preprocessed reviews into numerical vectors that work with machine learning algorithms. Model cannot process raw text data, so we can transform the data into numerically expressed information the model can work with. We can take CounterVectorizer from scikit-learn to implement a Bag-of-Words (BoW) approach. It converts a collection of documents into a matrix of term counts, each row represents a message and each column corresponds to a term (unigram or bigram). Before transformation, tokenization, vocabulary building, and the mapping of each document to a numeric vector occurs with CountVectorizer.\n",
    "\n",
    "This step prepares 'X' below to become a numerical feature matrix ready to be fed into a classifier like Naive Bayes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "394b4811-9c25-4c62-81be-49b3e093dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (24904, 10000)\n",
      "Shape of testing data: (24801, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize optimized CountVectorizer with bigrams and additional feature adjustments\n",
    "vectorizer = CountVectorizer(min_df=2, max_df=0.8, ngram_range=(1, 2), max_features=10000)\n",
    "\n",
    "# Fit and transform the 'text' column for training data\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"].dropna())\n",
    "\n",
    "# Transform the 'text' column for testing data\n",
    "X_test = vectorizer.transform(test_df[\"text\"].dropna())\n",
    "\n",
    "# Map labels to binary values for training and testing datasets\n",
    "y_train = (train_df[\"label\"].dropna() == 1).astype(int)\n",
    "y_test = (test_df[\"label\"].dropna() == 1).astype(int)\n",
    "\n",
    "# Output the shapes of the transformed data\n",
    "print(\"Shape of training data:\", X_train.shape)\n",
    "print(\"Shape of testing data:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63e914-eda0-4457-a0c1-cf0283cd0b4b",
   "metadata": {},
   "source": [
    "# Build the Sentiment Classification Model using MultinomialNB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d89fb-52c4-43aa-bd14-bdc7728e05ac",
   "metadata": {},
   "source": [
    "#### Building the Pipeline\n",
    "This will chain together vectorization using CountVectorizer and classification using MultinomialNB.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3917493-dcca-45e5-8c58-b4269a31db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create the pipeline for movie reviews with an optimized Naive Bayes classifier\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),  # Ensure vectorizer is appropriately configured (e.g., ngram_range, max_features)\n",
    "    (\"classifier\", MultinomialNB(alpha=0.75))  # Include a tuned alpha parameter for smoothing\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb67e3-3cfd-42da-b7aa-768a7b56a6da",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "This will be used to find the optimal value for 'alpha' parameter of MultinomialNB, with GridSearchCV in order to better generalize the model for classifying (1) positive and (0) negative for reviews.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8ac88bd-8a9d-4339-8054-cda5d36548b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best hyperparameters: {'classifier__alpha': 0.75}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Define the parameter grid for alpha tuning\n",
    "param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for balanced cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform the grid search with F1-score as metric and optimizations\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,  # Utilize all available CPU cores\n",
    "    verbose=3   # Enable detailed progress logging\n",
    ")\n",
    "\n",
    "# Train the grid search on the training data\n",
    "grid_search.fit(train_df[\"text\"], y_train)\n",
    "\n",
    "# Retrieve the best model and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ed27c-6722-4db6-a491-6983474d94de",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Now we can evaluate the models performance on the test set of movie reviews\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "485ba434-381e-4717-9929-476699dd0cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8271    0.8590    0.8428     12361\n",
      "           1     0.8543    0.8216    0.8376     12440\n",
      "\n",
      "    accuracy                         0.8402     24801\n",
      "   macro avg     0.8407    0.8403    0.8402     24801\n",
      "weighted avg     0.8408    0.8402    0.8402     24801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on the test data using the best model\n",
    "y_pred = best_model.predict(test_df[\"text\"])\n",
    "\n",
    "# Generate and print a classification report\n",
    "report = classification_report(y_test, y_pred, digits=4)  # Higher precision for metrics\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173aa15-4ae5-426a-98d9-3d519a59b8c2",
   "metadata": {},
   "source": [
    "#### Evaluating new movie reviews\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526e474-1dfb-4a03-8338-af11bdc06232",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = [\n",
    "    \"This movie was absolutely fantastic! A must-watch for everyone.\",\n",
    "    \"Terrible plot and awful acting. I would not recommend it to anyone.\",\n",
    "    \"A beautifully written storyline with stellar performances. I loved it!\",\n",
    "    \"It was just okay. Nothing special to write home about.\",\n",
    "    \"Horrible. Worst movie I have ever seen in my life.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac89ef-d726-4c53-aff0-51412b722261",
   "metadata": {},
   "source": [
    "#### Preprocess and Predict\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c893eec-337a-4bf6-835a-946f2c674944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie was absolutely fantastic! A must-watch for everyone.\n",
      "Prediction: Positive\n",
      "Positive Probability: 0.71\n",
      "Negative Probability: 0.29\n",
      "--------------------------------------------------\n",
      "Review: Terrible plot and awful acting. I would not recommend it to anyone.\n",
      "Prediction: Negative\n",
      "Positive Probability: 0.43\n",
      "Negative Probability: 0.57\n",
      "--------------------------------------------------\n",
      "Review: A beautifully written storyline with stellar performances. I loved it!\n",
      "Prediction: Positive\n",
      "Positive Probability: 0.75\n",
      "Negative Probability: 0.25\n",
      "--------------------------------------------------\n",
      "Review: It was just okay. Nothing special to write home about.\n",
      "Prediction: Negative\n",
      "Positive Probability: 0.24\n",
      "Negative Probability: 0.76\n",
      "--------------------------------------------------\n",
      "Review: Horrible. Worst movie I have ever seen in my life.\n",
      "Prediction: Negative\n",
      "Positive Probability: 0.08\n",
      "Negative Probability: 0.92\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments of the new reviews\n",
    "predictions = best_model.predict(new_reviews)\n",
    "prediction_probabilities = best_model.predict_proba(new_reviews)\n",
    "\n",
    "# Display results\n",
    "for i, review in enumerate(new_reviews):\n",
    "    prediction = \"Positive\" if predictions[i] == 1 else \"Negative\"\n",
    "    positive_prob = prediction_probabilities[i][1]\n",
    "    negative_prob = prediction_probabilities[i][0]\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Positive Probability: {positive_prob:.2f}\")\n",
    "    print(f\"Negative Probability: {negative_prob:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bee09-1649-4128-860a-f10c4a87bb00",
   "metadata": {},
   "source": [
    "# Save the Model for Submission\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93b1f37a-650f-447d-9f9c-c623f93fc222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to skills_assessment.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file for future use\n",
    "model_filename = 'skills_assessment.joblib'\n",
    "joblib.dump(best_model, model_filename)  # Save the model\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e9638-7a30-4383-b6ce-71a0fe3a266a",
   "metadata": {},
   "source": [
    "# Uploading the Model to Hack The Box's Endpoint for flag\n",
    "I cleared the output of the flag in order to encourage those who may land here to do the work themselves.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a9107-56d8-4b16-aa9d-ad3b0b5cef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL of the API endpoint\n",
    "url = \"http://10.129.151.56:5000/api/upload\"\n",
    "\n",
    "# Path to the model file you want to upload\n",
    "model_file_path = \"skills_assessment.joblib\"\n",
    "\n",
    "# Open the file in binary mode and send the POST request\n",
    "with open(model_file_path, \"rb\") as model_file:\n",
    "    files = {\"model\": model_file}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Pretty print the response from the server\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
