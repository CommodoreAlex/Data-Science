{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07eb2c07-feff-4a4e-b344-c5a25c757870",
   "metadata": {},
   "source": [
    "# Spam Classification with Naive Bayes Theorem\n",
    "\n",
    "## Overview\n",
    "This project focuses on classifying SMS messages as either spam or ham (legitimate) using the Naive Bayes Theorem. The goal is to build an efficient classifier that can distinguish unwanted messages from legitimate ones based on historical data.\n",
    "\n",
    "## Dataset\n",
    "We are utilizing the **SMS Spam Collection** dataset from the UC Irvine Machine Learning Repository:\n",
    "\n",
    "- **Dataset Link:** [SMS Spam Collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)\n",
    "- The dataset contains **5,574** SMS messages, each labeled as either **spam** or **ham**.\n",
    "- This dataset has been widely used in mobile phone spam research and serves as a strong benchmark for spam classification tasks.\n",
    "\n",
    "## Background\n",
    "The **Naive Bayes Theorem** is a probabilistic approach based on Bayes' Rule, assuming that the presence of one feature in a classification task is independent of the others. Despite its simplicity, it performs well in text classification tasks due to its ability to handle high-dimensional data effectively.\n",
    "\n",
    "### Why Naive Bayes?\n",
    "- **Fast and Efficient:** Works well with large text datasets.\n",
    "- **Performs well with small training data:** Requires less training time compared to deep learning models.\n",
    "- **Handles noisy data:** Suitable for spam detection where wording can be highly variable.\n",
    "\n",
    "## Methodology\n",
    "1. **Data Preprocessing**\n",
    "   - Load and clean the dataset.\n",
    "   - Tokenization, lowercasing, and removing unnecessary characters.\n",
    "   - Converting text into numerical feature vectors using techniques like TF-IDF or Count Vectorization.\n",
    "\n",
    "2. **Model Training**\n",
    "   - Implementing Naive Bayes classification (MultinomialNB or BernoulliNB from scikit-learn).\n",
    "   - Splitting the dataset into training and testing subsets.\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Measuring accuracy, precision, recall, and F1-score.\n",
    "   - Analyzing false positives and false negatives.\n",
    "\n",
    "4. **Deployment and Future Enhancements**\n",
    "   - Exploring additional preprocessing techniques to improve accuracy.\n",
    "   - Extending to real-world applications such as email spam filtering.\n",
    "\n",
    "## Conclusion\n",
    "This project demonstrates the effectiveness of the **Naive Bayes** approach in spam classification and highlights its practical applications in AI-driven security solutions. As part of the **AI Red Teamer Job Role Path**, understanding spam detection techniques strengthens adversarial AI defense mechanisms, preparing security professionals for real-world AI security challenges.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216d9ac-7183-4fef-9a74-503368538348",
   "metadata": {},
   "source": [
    "# Environment Setup and Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935b90a-4084-4501-bf4d-dca0ea9683bd",
   "metadata": {},
   "source": [
    "#### Download the Dataset\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "823e5622-715b-445b-9095-e2f9ecb19555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Successful.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Download Successful.\")\n",
    "else:\n",
    "    print(\"Failed to download the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d66338-a3f0-4820-b465-65b1ad888721",
   "metadata": {},
   "source": [
    "#### Extracting the DataSet\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c00c23-3155-48d8-b446-40f87a8006b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction Complete\n",
      "Extracted files:  ['readme', 'SMSSpamCollection']\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"sms_spam_collection\")\n",
    "    print(\"Extraction Complete\")\n",
    "\n",
    "# Verify complete extraction\n",
    "import os\n",
    "extracted_files = os.listdir(\"sms_spam_collection\")\n",
    "print(\"Extracted files: \", extracted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff83c9fc-27c4-407d-92f8-ce6facb6a3d7",
   "metadata": {},
   "source": [
    "#### Loading the DataSet\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e308a87-b1f2-4ca1-9e6f-d5c26e68d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\n",
    "    \"sms_spam_collection/SMSSpamCollection\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"message\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56d14556-b487-4c64-a2ae-cccaf3d1f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- HEAD --------------------\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "-------------------- DESCRIBE --------------------\n",
      "       label                 message\n",
      "count   5572                    5572\n",
      "unique     2                    5169\n",
      "top      ham  Sorry, I'll call later\n",
      "freq    4825                      30\n",
      "-------------------- INFO --------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Displaying basic information about the dataset\n",
    "print(\"-------------------- HEAD --------------------\")\n",
    "print(df.head()) # First few rows\n",
    "print(\"-------------------- DESCRIBE --------------------\")\n",
    "print(df.describe()) # Statistical Summary\n",
    "print(\"-------------------- INFO --------------------\")\n",
    "print(df.info()) # Concise summary, including non-null entry number and data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df02fd04-f8f0-4438-aebd-1665325ecdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missingvalues:\n",
      " label      0\n",
      "message    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values that can skew and reduce the quality of data\n",
    "print(\"Missingvalues:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f28a653f-dff6-4d29-af07-6eea0c34be9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries:\n",
      " 403\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate values for the same purpose\n",
    "print(\"Duplicate entries:\\n\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3599ba82-aed6-4c46-8124-921eb2c16c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicates (you can run the above to confirm)\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75becff9-d03c-4e98-a745-b7facd86cce3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Preprocessing the DataSet\n",
    "\n",
    "This is going to standardize the text, reduce noise, extract meaningful features (all improve performance), relying on the NLTK library for tokenization, stop word removal, and stemming to better implement the Bayes Spam Classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9a41379-7448-4cc4-8621-361aedc6a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ANY PREPROCESSINGM ===\n",
      "  label                                            message\n",
      "0   ham  go until jurong point, crazy.. available only ...\n",
      "1   ham                      ok lar... joking wif u oni...\n",
      "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n",
      "3   ham  u dun say so early hor... u c already then say...\n",
      "4   ham  nah i don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "#nltk.download(\"punkt\") # tokenization\n",
    "#nltk.download(\"punkt_tab\")\n",
    "#nltk.download(\"stopwords\") # Stop words\n",
    "\n",
    "print(\"=== BEFORE ANY PREPROCESSINGM ===\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a07c1-3ded-4c48-8d0e-e52348e35786",
   "metadata": {},
   "source": [
    "#### Lowercasing the text for word equality\n",
    "Regardless of original casing, this will consider \"Word\" and \"word\" as the same token, improving performance and uniformity across the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00048cb6-cae5-4c4b-b19f-4d2a901854da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER LOWERCASING ===\n",
      "0    go until jurong point, crazy.. available only ...\n",
      "1                        ok lar... joking wif u oni...\n",
      "2    free entry in 2 a wkly comp to win fa cup fina...\n",
      "3    u dun say so early hor... u c already then say...\n",
      "4    nah i don't think he goes to usf, he lives aro...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert all message text to lowercased\n",
    "df[\"message\"] = df[\"message\"].str.lower()\n",
    "print(\"\\n=== AFTER LOWERCASING ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b8151-0581-4c74-8999-29a35c356f52",
   "metadata": {},
   "source": [
    "#### Remove Punctuation and Numbers\n",
    "Simplify the dataset by focusing on meaningful words. Symbols such as '$' and '!' could be important for spam messages, be it a monetary amount of added emphasis.\n",
    "\n",
    "The code will remove all characters besides lowercase text, whitespace, dollar signs, and exclamation marks to make a more distinguishable dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "821d7c5d-d1c3-48cb-a786-98d8c184a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
      "0    go until jurong point crazy available only in ...\n",
      "1                              ok lar joking wif u oni\n",
      "2    free entry in  a wkly comp to win fa cup final...\n",
      "3          u dun say so early hor u c already then say\n",
      "4    nah i dont think he goes to usf he lives aroun...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbacf62-0884-4e23-8641-6f44b0f665bf",
   "metadata": {},
   "source": [
    "#### Tokenizing the Text\n",
    "We do this to divide the message text into individual words (tokens), before proceeding. We convert unstructured text to a sequence of words to prepare for removing stop words and applying stemming.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69068863-f465-4295-85c1-80c3e817b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER TOKENIZATION ===\n",
      "0    [go, until, jurong, point, crazy, available, o...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
      "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
      "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split each message into individual tokens\n",
    "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
    "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d228a7-3d9f-41d0-a71f-9834a3b39345",
   "metadata": {},
   "source": [
    "#### Removing Stop Words\n",
    "Stop words like 'and', 'the', 'is', that do not add meaningful context are removed to reduce noise from the dataset. This helps to distinguish SPAM (bad) vs HAM (good) and helps the model learn better. The token list is shorter after removing stop words, which creates a better dataset for future text transformations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "129d337e-8cb1-4410-82ce-7daaeda3a89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING STOP WORDS ===\n",
      "0    [go, jurong, point, crazy, available, bugis, n...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
      "3        [u, dun, say, early, hor, u, c, already, say]\n",
      "4    [nah, dont, think, goes, usf, lives, around, t...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a set of English stop words and remove them from the tokens\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c09c6-b5bc-43b9-877d-f01913797339",
   "metadata": {},
   "source": [
    "#### Stemming the Text\n",
    "Stemming normalizes words by reducing them to base form (e.g. eating becomes eat). So we are acquiring the 'root word', base of the word. Chopping up vocabulary to smooth out the text representation, so the model can learn better and understand what is going on without too much variance. This improves our models ability to generalize.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5134588d-0c7c-43b1-82e2-b4a394b685a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER STEMMING ===\n",
      "0    [go, jurong, point, crazi, avail, bugi, n, gre...\n",
      "1                         [ok, lar, joke, wif, u, oni]\n",
      "2    [free, entri, wkli, comp, win, fa, cup, final,...\n",
      "3        [u, dun, say, earli, hor, u, c, alreadi, say]\n",
      "4    [nah, dont, think, goe, usf, live, around, tho...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Stem each token to reduce words to their base form\n",
    "stemmer = PorterStemmer()\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(\"\\n=== AFTER STEMMING ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb224c0-bbac-4c4c-a13c-68a5e5c7efae",
   "metadata": {},
   "source": [
    "#### Joining Tokens Back into a Single String\n",
    "Machine learning algorithms and vectorization techniques (e.g. TF_IDF) work better with raw strings. Rejoining tokens into a space-separated string restores a format compatible with these methods, preparing the dataset for the feature extraction phase.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ce668744-451d-4d58-8e2a-067b1d3ead27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
      "0    go jurong point crazi avail bugi n great world...\n",
      "1                                ok lar joke wif u oni\n",
      "2    free entri wkli comp win fa cup final tkt st m...\n",
      "3                  u dun say earli hor u c alreadi say\n",
      "4            nah dont think goe usf live around though\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rejoin tokens into a single string for feature extraction\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
    "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ee90f-4eae-4874-844e-ac0f17999fa2",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "The purpose of feature extraction is to transform preprocessed SMS messages into numerical vectors that work with machine learning algorithms. Models cannot directly process raw text data, so we transform the data into numerically represented information the model can consume.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a51fa-3f07-4a5b-bf97-3d9be7ba3812",
   "metadata": {},
   "source": [
    "#### Counter Vectorization for the Bag-of-Words Approach\n",
    "CountVectorizer can be used from the Scikit-learn library to implement a bag-of-words approach. It converts a collection of documents into a matrix of term counts, each row represents a message and each column corresponds to a term (unigram or bigram). Before transformation, tokenization, vocabulary building, and the mapping of each document to a numeric vector occurs with CountVectorizer.\n",
    "\n",
    "This step prepares 'X' below to become a numerical feature matrix ready to be fed into a classifier like Naive Bayes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1f0f77ef-aa40-40bf-913b-d6e62d3c1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the message column\n",
    "X = vectorizer.fit_transform(df[\"message\"])\n",
    "\n",
    "# Labels (target variable)\n",
    "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0) # Converting labels 1 to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30de984-d48b-44cd-aac8-41f43a201533",
   "metadata": {},
   "source": [
    "# Training and Evaluation (Spam Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad2649-7b52-41a7-9b46-5bfc10bb743b",
   "metadata": {},
   "source": [
    "#### Training using a 'Multinomial Naive Bayes' classifier\n",
    "We employ a pipeline to chain together vectorization and modeling steps, ensuring data transformation is applied before feeding the transformed data into the classifier. This encapsulates the feature extraction and model training into a single workflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "247a2e02-32e5-4b20-874a-3fef37ec1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build the pipeline by combining vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7435f465-d8ce-47f9-a36e-7a46926723d6",
   "metadata": {},
   "source": [
    "#### Integrating Hyperparamter Tuning for Performance Boost\n",
    "GridSearchCV will allow us to find optimal paramter values for the classifier, that ensures the model generalizes well and avoids overfitting. This will balance bias and variance by tuning 'alpha', a smoothing factor that adjusts how the model handles unseen words and prevents possibilities from being zero, to improve the model's robustness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4bba59a6-867f-43d8-9d99-6ab144b2b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'classifier__alpha': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "# Fit the grid search on the full dataset\n",
    "grid_search.fit(df[\"message\"], y)\n",
    "\n",
    "# Extract the best model identified by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a731fc1-9bdf-496a-b920-80066be44644",
   "metadata": {},
   "source": [
    "#### Setting up Evaluation Messages\n",
    "We provide a list of new SMS messages for evaluation. These messages represent the type of inputs the model may receive in real-world use, including different types of SPAM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5ed4b08d-9cfb-4ff6-a495-3f25fe77c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example SMS messages for evaluation\n",
    "new_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
    "    \"Hey, are we still meeting up for lunch today?\",\n",
    "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
    "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
    "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf251a-9e6b-46b0-a3d0-2b001026ff09",
   "metadata": {},
   "source": [
    "#### Preprocessing New Messages\n",
    "Before predicting with the model we must preprocess new messages using hte same steps applied during training. We can create a preprocess_message function will take care of this for us.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "00d25d00-8637-44ad-af0e-623b66d7d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Preprocess functio nthat mirrors the training-time preprocessing\n",
    "def preprocess_message(message):\n",
    "    message = message.lower()\n",
    "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
    "    tokens = word_tokenize(message)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Applying this function to preprocess and vectorize messages\n",
    "processed_messages = [preprocess_message(msg) for msg in new_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c7d44-a8ce-4f8e-807c-216a75f0ad55",
   "metadata": {},
   "source": [
    "#### Vectorizing the Processed Messages\n",
    "The model requires numerical input features, so we need to apply the same vectorization methods used during trianing. The CountVectorizer saved within the pipeline is available to do this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cc20ba3b-58ba-4c25-9003-16239e7620b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform preprocessed messages into feature vectors\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415d34c-897c-47f9-801d-f81c5f5ee88d",
   "metadata": {},
   "source": [
    "#### Making Predictions\n",
    "With the data preprocessed and vectorized, we can feed the new messages into the trained MultinomialNB classifier to output both a predicted lable (spam or not spam) and class probability, indicating the model's confidence in its decisions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43b00fa3-b5c7-475e-82fe-5967e596791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the trained classifier\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea9938e-5b16-485c-9ae0-80827de9ac5b",
   "metadata": {},
   "source": [
    "#### Displaying Predictions and Probabilities\n",
    "We can now present the evaluation results and display the following:\n",
    "- The original text of the message.\n",
    "- The predicted label (Spam or Not-Spam).\n",
    "- The probability that the message is spam.\n",
    "- The probability that the message is not spam.\n",
    "\n",
    "These results show that the model can successfully determine SPAM vs real data.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3fbd3eab-7bae-4943-ab10-b3c201ef2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
      "Prediction: Spam\n",
      "Spam Probability: 1.00\n",
      "Not-Spam Probability: 0.00\n",
      "--------------------------------------------------\n",
      "Message: Hey, are we still meeting up for lunch today?\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
      "Prediction: Spam\n",
      "Spam Probability: 0.96\n",
      "Not-Spam Probability: 0.04\n",
      "--------------------------------------------------\n",
      "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
      "Prediction: Spam\n",
      "Spam Probability: 1.00\n",
      "Not-Spam Probability: 0.00\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display predictions and probabilities for each evaluated message\n",
    "for i, msg in enumerate(new_messages):\n",
    "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
    "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
    "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
    "    \n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
    "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db77798-957c-4137-80f4-b6ad59e017f7",
   "metadata": {},
   "source": [
    "#### Saving Models with JobLib\n",
    "We can preserve the model for re-use later by saving it to a file. Joblib is a Python library designed to serialize and deserialize Python objects, like large arrays (NumPy) or Scikit-learn models.\n",
    "\n",
    "\"Serialization converts an in-memory object into a format that can be stored on disk or transmitted across networks. Deserialization involves converting the stored representation back into an in-memory object with the exact same state it had when saved.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "efa2a01f-ed17-4a78-aab2-5fb89c615101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to spam_detection_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file for future use\n",
    "model_filename = 'spam_detection_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c378c87b-e800-48fc-84fb-a795c2c5e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can reuse the model later, by submitting\n",
    "#loaded_model = joblib.load(model_filename)\n",
    "#predictions = loaded_model.predict(new_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6f0f8-0c1d-4c68-b171-86307e300f0f",
   "metadata": {},
   "source": [
    "# Uploading the Model to Hack The Box's Endpoint for flag\n",
    "I cleared the output of the flag in order to encourage those who may land here to do the work themselves.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e3b86-c65a-495a-9b04-41a540dc5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL of the API endpoint\n",
    "url = \"http://10.129.51.30:8000/api/upload\"\n",
    "\n",
    "# Path to the model file you want to upload\n",
    "model_file_path = \"spam_detection_model.joblib\"\n",
    "\n",
    "# Open the file in binary mode and send the POST request\n",
    "with open(model_file_path, \"rb\") as model_file:\n",
    "    files = {\"model\": model_file}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Pretty print the response from the server\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
