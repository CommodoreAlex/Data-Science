{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87be9651-7660-4d57-be8a-3f47b9f7ce22",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Python\n",
    "\n",
    "Welcome to my Jupyter notebook, where I share my learning journey in Natural Language Processing (NLP) with Python. In this notebook, I'll walk through key concepts, techniques, and libraries, and share hands-on examples as I dive into my first experiences with NLP.\n",
    "\n",
    "## What is Natural Language Processing (NLP)?\n",
    "\n",
    "NLP is a subfield of artificial intelligence that enables computers to understand, interpret, and generate human language. Key tasks in NLP include:\n",
    "- Text analysis\n",
    "- Sentiment analysis\n",
    "- Named Entity Recognition (NER)\n",
    "- Language translation\n",
    "- Text summarization\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will explore key NLP techniques and tools, primarily focusing on Python’s `NLTK` library. Key concepts include:\n",
    "- **Segmentation**: Dividing text into smaller chunks (e.g., sentences or words).\n",
    "- **Tokenization**: Converting text into meaningful units (tokens).\n",
    "- **Stop Words**: Removing common, non-essential words.\n",
    "- **Stemming & Lemmatization**: Reducing words to their root or base forms.\n",
    "- **Part-of-Speech (POS) Tagging**: Assigning word classes to tokens.\n",
    "- **Named Entity Recognition (NER)**: Extracting and classifying entities (e.g., people, organizations, locations).\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How to segment text into sentences and words.\n",
    "2. Tokenization techniques for breaking down text.\n",
    "3. How to remove stop words to focus on meaningful terms.\n",
    "4. The differences between stemming and lemmatization.\n",
    "5. POS tagging and how it classifies words in context.\n",
    "6. NER for extracting entities from text.\n",
    "\n",
    "## Resources\n",
    "\n",
    "For further learning, explore:\n",
    "1. [GeeksforGeeks: Part of Speech Tagging](https://www.geeksforgeeks.org/nlp-part-of-speech-default-tagging/)\n",
    "2. [YouTube: NLP Overview and Basics](https://www.youtube.com/watch?v=MpIagClRELI)\n",
    "3. [YouTube: Additional NLP Insights](https://www.youtube.com/watch?v=fLvJ8VdHLA0&list=PLhBFZf0L5I7qN_qb4P1lvrjhHtMd7403_&index=18)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e87571-6638-4ef9-a0cd-a760f872d2b5",
   "metadata": {},
   "source": [
    "#### 1) Segmentation\n",
    "Segmentation in NLP refers to dividing text into meaningful units, such as sentences, words, or topics. It's a foundational step for further processing and analysis. Common types of segmentation include:\n",
    "- Sentence Segmentation: Splitting a document into sentences using punctuation and context.\n",
    "- Word Segmentation: Breaking down sentences into words, crucial for languages without clear word boundaries (e.g., Chinese).\n",
    "- Topic Segmentation: Dividing a document into sections based on themes or topics for better understanding.\n",
    "\n",
    "Segmentation helps NLP systems organize and analyze text more effectively for tasks like summarization, translation, or sentiment analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68130bc4-b91b-4fbd-a455-5135757c4312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below is a set of text that I will be working with in order to test NLP methods\n",
    "text = \"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685b9fb-99a7-4880-b494-29e15c471e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries (NLTK)\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4589517e-1f86-4ccc-a562-6ac18463bd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry.',\n",
       " 'The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066.',\n",
       " 'Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace.',\n",
       " \"Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the text into sentences with PUNKT\n",
    "# Punkt is a tokenizer that includes pre-trained data for different languages to help with splitting text into sentences.\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1dd36c0e-4143-4c8b-9b7b-3ec0f9054b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling separated parts of the text data (elements, via index positions)\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7862869-9cb7-4b1f-8224-f84d7352b388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace '"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation removal with the RE library\n",
    "import re\n",
    "\n",
    "# Removing punctuation characters\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", sentences[2])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfde1e-9725-45c8-a224-3ee0f21be7e3",
   "metadata": {},
   "source": [
    "#### 2) Tokenization\n",
    "Tokenization is the process of breaking down a text into smaller units called \"tokens.\" Tokens can be words, phrases, or even individual characters, depending on the use case. For example:\n",
    "- Sentence Tokenization: Splits text into sentences.\n",
    "- Word Tokenization: Breaks sentences into words.\n",
    "- Character Tokenization: Divides words into individual characters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "46c03e10-cb5b-4a52-8884-84816f156f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libaries\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b6614bc9-91ca-4b23-8fb6-9141b7ce1edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queen', 'Camilla', 'was', 'crowned', 'alongside', 'him', 'before', 'a', 'huge', 'parade', 'back', 'to', 'Buckingham', 'Palace']\n"
     ]
    }
   ],
   "source": [
    "# Creating a list with each element corresponding to a separate word\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14141576-5ac2-4c65-8403-382314c74171",
   "metadata": {},
   "source": [
    "#### 3) Removal of Stop Words\n",
    "Stop words are common words like \"a,\" \"the,\" and \"in\" that occur frequently in text but carry minimal meaning. In NLP, these words are often removed to focus on content-rich terms that improve analysis and efficiency. However, their removal depends on the task—sometimes they contribute to sentence structure or context, like in sentiment analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d59eb0c9-fafa-4515-af3c-0368ee22c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1b24526f-1d83-4544-95ec-d72f9200db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queen', 'Camilla', 'crowned', 'alongside', 'huge', 'parade', 'back', 'Buckingham', 'Palace']\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "65f49e44-69a7-4e70-970e-6f14db6292b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "# You can see the stop words in NLTK's corpus (The NLTK corpus is a massive dump of all kinds of natural language data sets)\n",
    "print(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770f1a7-57ff-4bc1-92e2-d8ae12f88f0e",
   "metadata": {},
   "source": [
    "#### 4) Stemming and Lemmetization\n",
    "Lemmatization is the process of reducing a word to its base or dictionary form, known as its \"lemma,\" while considering the word's context and part of speech. For example, the words \"running\" and \"ran\" are both reduced to the lemma \"run.\" Unlike stemming, which simply trims word endings, lemmatization ensures the result is a valid word, making it more accurate for NLP tasks like text classification, search, or sentiment analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "675a9a09-7716-45ba-b3f5-6e983ad2526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['queen', 'camilla', 'crown', 'alongsid', 'huge', 'parad', 'back', 'buckingham', 'palac']\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries for stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a031183d-205c-4e37-8066-5a0194ae9d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queen', 'Camilla', 'crowned', 'alongside', 'huge', 'parade', 'back', 'Buckingham', 'Palace']\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries for lemmatization\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ce014d9c-4b3e-49b5-a246-a71ce2906544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming output: ['wait', 'wait', 'studi', 'studi', 'comput']\n",
      "Lemmatization output: ['wait', 'waiting', 'study', 'studying', 'computer']\n"
     ]
    }
   ],
   "source": [
    "# Another stemming and lemmatization example\n",
    "words2 = ['wait', 'waiting', 'studies', 'studying', 'computers']\n",
    "\n",
    "# Stemming- Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words2]\n",
    "print(\"Stemming output: {}\".format(stemmed))\n",
    "\n",
    "# Lemmatization- Reduce words to their root form\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words2]\n",
    "print(\"Lemmatization output: {}\".format(lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60ea36-ec1c-4604-ba66-9971a34267ae",
   "metadata": {},
   "source": [
    "#### 5) Part of Speech (POS) Tagging\n",
    "Part-of-speech (POS) tagging assigns a grammatical category, like noun, verb, or adjective, to each word in a sentence based on its context. For example, in \"She runs fast,\" \"runs\" is tagged as a verb, while in \"She bought new running shoes,\" \"running\" is tagged as an adjective. POS tagging helps NLP systems understand sentence structure and meaning, aiding in tasks like syntax parsing, sentiment analysis, and machine translation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647dbbc4-7236-477a-86cd-9c427e057254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f6b70c67-9ec6-425b-b850-e005c54b6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2a6c8e12-db5e-444c-a443-946fa0cfa42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Queen', 'NNP'),\n",
       " ('Camilla', 'NNP'),\n",
       " ('crowned', 'VBD'),\n",
       " ('alongside', 'RB'),\n",
       " ('huge', 'JJ'),\n",
       " ('parade', 'NN'),\n",
       " ('back', 'RB'),\n",
       " ('Buckingham', 'NNP'),\n",
       " ('Palace', 'NNP')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag each word with part of speech\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5887fe-2c51-4f61-9c53-70b401fe0003",
   "metadata": {},
   "source": [
    "You can find the tagging labels here: https://www.geeksforgeeks.org/nlp-part-of-speech-default-tagging/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad9eaf-4181-402a-9ef0-02a2bed1c7b9",
   "metadata": {},
   "source": [
    "#### 5) Named Entity Recognition\n",
    "Named Entity Recognition (NER) is the process of identifying and categorizing specific entities in text, such as names of people, organizations, locations, dates, and more. For example, in \"Bill Gates founded Microsoft in 1975,\" \"Bill Gates\" is identified as a person, \"Microsoft\" as an organization, and \"1975\" as a date. NER helps NLP systems extract valuable information for tasks like information retrieval, chatbots, and summarization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848d3d2-d531-4324-8329-1edeb72fdbaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5e061379-16a9-4106-82ef-3c36b3ad9fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Queen/NNP)\n",
      "  (PERSON Camilla/NNP)\n",
      "  was/VBD\n",
      "  crowned/VBN\n",
      "  alongside/RB\n",
      "  him/PRP\n",
      "  before/IN\n",
      "  a/DT\n",
      "  huge/JJ\n",
      "  parade/NN\n",
      "  back/RB\n",
      "  to/TO\n",
      "  (PERSON Buckingham/NNP Palace/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# This performs NER by tokenizing the third sentence, tagging words with their parts of speech, \n",
    "# identifying named entities (like people or places), and outputting them in a structured tree format.\n",
    "ner_tree = ne_chunk(pos_tag(word_tokenize(sentences[2])))\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2a3690c8-dc39-46e7-bf64-6430916e5f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Queen/NNP)\n",
      "  (PERSON Camilla/NNP)\n",
      "  was/VBD\n",
      "  crowned/VBN\n",
      "  alongside/RB\n",
      "  him/PRP\n",
      "  before/IN\n",
      "  a/DT\n",
      "  huge/JJ\n",
      "  parade/NN\n",
      "  back/RB\n",
      "  to/TO\n",
      "  (PERSON Buckingham/NNP Palace/NNP))\n"
     ]
    }
   ],
   "source": [
    "# We can run this against the whole 'data set' to see what it expands on\n",
    "ner_tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "53bf1425-c65d-4a31-92d6-1c10e08181f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION CEO/NNP)\n",
      "  of/IN\n",
      "  (GPE NVIDIA/NNP)\n",
      "  ,/,\n",
      "  (PERSON Jensen/NNP Huang/NNP)\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  on/IN\n",
      "  April/NNP\n",
      "  5th/CD\n",
      "  ,/,\n",
      "  1993/CD\n",
      "  and/CC\n",
      "  is/VBZ\n",
      "  62/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  !/.)\n"
     ]
    }
   ],
   "source": [
    "# This gives a better understanding of how NER identifies and points out- person, organization, role, dates, etc.\n",
    "new_text = \"The CEO of NVIDIA, Jensen Huang, was born on April 5th, 1993 and is 62 years old!\"\n",
    "ner_tree = ne_chunk(pos_tag(word_tokenize(new_text)))\n",
    "print(ner_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
