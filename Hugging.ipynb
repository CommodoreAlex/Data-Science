{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6f5023-d074-4168-a014-428024d0c367",
   "metadata": {},
   "source": [
    "# Utilizing Hugging Faces Pipelines for Natural Language Processing (NLP)\n",
    "\n",
    "## Project Overview  \n",
    "\n",
    "As a **data science student**, Iâ€™m exploring **Hugging Face** and its capabilities, sharing insights on how to leverage these powerful tools for NLP. This project covers:  \n",
    "\n",
    "### Key Topics  \n",
    "- **Pipelines** â€“ Abstracting complex NLP tasks into a few lines of code  \n",
    "- **Tokenizers & Models** â€“ Understanding how text is processed  \n",
    "- **PyTorch** â€“ Running models with your preferred framework  \n",
    "- **Saving & Loading Models** â€“ Managing models efficiently  \n",
    "\n",
    "### What is Hugging Face? ðŸ¤—  \n",
    "\n",
    "Hugging Face is a leading platform in **Natural Language Processing (NLP)** and **Machine Learning (ML)**, providing state-of-the-art transformer models, tools, and an open-source ecosystem for working with AI. It simplifies complex deep learning tasks and allows users to apply powerful models with minimal effort.  \n",
    "\n",
    "Learn more about the [pipelines in Hugging Face](https://huggingface.co/docs/transformers/en/main_classes/pipelines), where you can see a comprehensive list here of all the pipelines you can call from Hugging Face.\n",
    "\n",
    "See a comprehensive list of\n",
    "\n",
    "There is an entire course for Hugging face here: [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)\n",
    "\n",
    "You can see the complete list of models, in the [Model Hub](https://huggingface.co/models)\n",
    "\n",
    "### Why is Hugging Face Valuable?  \n",
    "\n",
    "- **Ease of Use** â€“ Apply advanced NLP models with just a few lines of code.  \n",
    "- **Pre-trained Models** â€“ Access thousands of ready-to-use models from the **Model Hub**, reducing training time and cost.  \n",
    "- **Interoperability** â€“ Works seamlessly with **PyTorch**, **TensorFlow**, and **JAX**.  \n",
    "- **Fine-tuning & Customization** â€“ Train models on your own data for domain-specific applications.  \n",
    "- **Open-Source & Community-Driven** â€“ Supported by a vast community contributing to continuous improvements.  \n",
    "\n",
    "### What Are Transformers?  \n",
    "\n",
    "**Transformers** are a type of deep learning model that have revolutionized **Natural Language Processing (NLP)**. Introduced in the paper *\"Attention Is All You Need\"* by **Vaswani et al. (2017)**, transformers use a mechanism called **self-attention** to process words in relation to all other words in a sentence, rather than sequentially like traditional models.  \n",
    "\n",
    "### Why Are Transformers Powerful?  \n",
    "\n",
    "- **Parallelization** â€“ Unlike older models (e.g., RNNs), transformers process words **simultaneously**, making them highly efficient.  \n",
    "- **Context Awareness** â€“ They capture the **meaning of words** based on their **entire context**, rather than just nearby words.  \n",
    "- **Scalability** â€“ Transformers power massive models like **GPT-4, BERT, and T5**, capable of human-like text understanding and generation.  \n",
    "- **Versatility** â€“ Used for **text generation, translation, question answering, summarization, and more!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514cd83-be02-4e30-9549-8e3106a1f1cc",
   "metadata": {},
   "source": [
    "#### Installing Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c648c1-17b0-49db-a301-3efb3505d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing hugging face transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53c2c5-6e26-427b-9f4b-e425d684b97c",
   "metadata": {},
   "source": [
    "#### 1) Abstracting away NLP tasks with Hugging Face's 'Pipelines'\n",
    "This demonstrates the power of Hugging Face's pipeline abstraction, which simplifies working with complex NLP models. By simply specifying \"sentiment-analysis\", we load a pre-trained model that handles text classification without needing to fine-tune or preprocess data manually.\n",
    "\n",
    "Hugging Face provides various pipelines for different NLP tasks, such as:\n",
    "- \"text-generation\" for AI-powered writing\n",
    "- \"translation\" for multilingual applications\n",
    "- \"ner\" (Named Entity Recognition) for extracting key information\n",
    "- \"question-answering\" for answering queries from context\n",
    "- \n",
    "This abstraction makes NLP accessible, allowing developers to quickly experiment with state-of-the-art models without deep ML expertise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a744ae3-30f5-49ea-8bdb-e75caeeb3cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9987579584121704}]\n"
     ]
    }
   ],
   "source": [
    "# Importing the pipeline function from Hugging Face's transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initializing a sentiment-analysis pipeline using a pre-trained model.\n",
    "# By default, the pipeline function will use a pre-trained model for sentiment analysis, which is typically fine-tuned on sentiment classification tasks.\n",
    "# The model used here will likely be a binary sentiment model (positive/negative).\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Running the classifier on a sample text (a joke here) to analyze its sentiment.\n",
    "# The input text will be processed, and the model will classify its sentiment as either positive or negative.\n",
    "result = classifier(\"Why did the astronaut break up with the alien? Because the relationship needed more space!\")\n",
    "\n",
    "# Printing the result, which contains the model's sentiment prediction (label and confidence score).\n",
    "# The output will include the sentiment label (e.g., \"POSITIVE\" or \"NEGATIVE\") and the model's confidence score, indicating how sure the model is about its prediction.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d10556-5ad5-4fc7-a0b4-0853f959d08d",
   "metadata": {},
   "source": [
    "#### 2) Using the Text-Generation Pipeline and Selecting a Pre-Trained Model\n",
    "By selecting a pre-trained model, such as DistilGPT-2, a distilled (smaller and faster) version of GPT-2, we can generate contextually relevant and coherent text based on an input prompt.\n",
    "\n",
    "The text-generation pipeline allows you to:\n",
    "- Initialize the generator with a pre-trained model.\n",
    "- Provide an input prompt that serves as the basis for generating text.\n",
    "- Control the length of the generated text (max_length).\n",
    "- Generate multiple variations of the generated text (num_return_sequences).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10be43c8-f6b0-48e7-ba0d-309ef6551e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"The President of the United States is writing a bill about the future of our nation's most powerful financial system. And I'm in favor of what we\"}, {'generated_text': 'The President of the United States is writing a bill about how we can strengthen our law and protect our democracy. The bill proposes to change federal laws for'}]\n"
     ]
    }
   ],
   "source": [
    "# Initializing a text-generation pipeline using a pre-trained model.\n",
    "# The 'distilgpt2' model is a smaller, distilled version of GPT-2, designed for text generation.\n",
    "# It generates coherent and contextually relevant text based on the input prompt.\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# Running the text generator on the provided prompt.\n",
    "# The model will generate new text based on the starting sentence \"The President of the United States is writing a bill about\".\n",
    "# Parameters:\n",
    "# - max_length=30: The maximum length of the generated text (including the input prompt).\n",
    "# - num_return_sequences=2: The number of different text completions to generate.\n",
    "result = generator(\n",
    "    \"The President of the United States is writing a bill about\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "\n",
    "# Printing the result, which contains the generated text based on the input prompt.\n",
    "# The result will include two generated sequences since num_return_sequences=2.\n",
    "# Each sequence will have a maximum length of 30 tokens (including the prompt).\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf72d5b-f2ef-4451-91c6-d8a78b594305",
   "metadata": {},
   "source": [
    "#### 3) Expanding on the Previous Idea with Zero-Shot Classification\n",
    "Zero-shot classification is a machine learning technique where a model is able to classify data into categories it has never seen during training. Instead of training on a specific set of labels, the model is provided with a set of candidate labels and can classify new, unseen data based on its understanding of the relationships between the input text and those labels.\n",
    "\n",
    "This is especially useful when you donâ€™t have labeled data for every possible category or when new categories arise after training, allowing the model to make predictions without needing additional training on those specific labels. In the context of Hugging Face, it leverages a pre-trained model (like BART or RoBERTa) to perform this task efficiently.\n",
    "\n",
    "Notice the highest score, which should be 'politics', is mostly correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bf79ab9-d7a3-4049-9268-91adc5234b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'The President of the United States is writing a bill about', 'labels': ['government', 'law', 'politics'], 'scores': [0.5303876996040344, 0.28375473618507385, 0.18585756421089172]}\n"
     ]
    }
   ],
   "source": [
    "# Importing the pipeline function from Hugging Face's transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initializing a zero-shot classification pipeline.\n",
    "# The 'zero-shot-classification' task allows the model to classify text into categories, even if it has not been explicitly trained on those categories.\n",
    "# This is useful when you have a text but don't have a model specifically fine-tuned for your categories.\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Running the classifier on a sample text to classify its topic.\n",
    "# The input text (\"The President of the United States is writing a bill about\") is passed to the model for classification.\n",
    "# Candidate labels are the potential categories the text could belong to: \"government\", \"politics\", and \"law\".\n",
    "result = classifier(\n",
    "    \"The President of the United States is writing a bill about\",\n",
    "    candidate_labels=[\"government\", \"politics\", \"law\"],\n",
    ")\n",
    "\n",
    "# Printing the result, which contains the classification output.\n",
    "# The result will indicate which candidate label the model predicts is the most appropriate for the input text,\n",
    "# along with a confidence score for the classification.\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342833c9-bdff-4375-bf3c-ef637e5edae2",
   "metadata": {},
   "source": [
    "#### 4) Using Tokenizers with Hugging Face Sentiment Analysis\n",
    "\n",
    "A tokenizer basically puts a text into a mathematical representation that the model can understand. This code demonstrates how to use Hugging Face's tokenizers and models for sentiment analysis and tokenization tasks. It shows how to load pre-trained models, tokenize a sequence, convert tokens to IDs, and decode them back to text.\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0490b05-48ad-45f9-9e11-46e4e49480ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9987579584121704}]\n",
      "{'input_ids': [101, 19081, 2024, 2428, 4658, 1010, 1998, 2025, 2061, 3697, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['transformers', 'are', 'really', 'cool', ',', 'and', 'not', 'so', 'difficult']\n",
      "[19081, 2024, 2428, 4658, 1010, 1998, 2025, 2061, 3697]\n",
      "transformers are really cool, and not so difficult\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries for Tokenizer and model loading.\n",
    "# AutoTokenizer and AutoModelForSequenceClassification are used for loading pre-trained models and tokenizers from Hugging Face.\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initializing a sentiment-analysis pipeline using a pre-trained model (default model for sentiment-analysis).\n",
    "# The pipeline function handles the preprocessing, model inference, and post-processing for sentiment analysis.\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Running the classifier on a sample joke to analyze its sentiment.\n",
    "# The model will classify the sentiment as either POSITIVE or NEGATIVE.\n",
    "result = classifier(\"Why did the astronaut break up with the alien? Because the relationship needed more space!\")\n",
    "print(result)\n",
    "\n",
    "# Specifying the model name for tokenization and classification.\n",
    "# The model 'distilbert-base-uncased-finetuned-sst-2-english' is a pre-trained DistilBERT model fine-tuned on the SST-2 dataset for sentiment analysis.\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Loading the pre-trained model for sequence classification (sentiment analysis).\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Loading the tokenizer associated with the model.\n",
    "# The tokenizer is used to convert the input text into tokens that the model can process.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenizing the input text sequence into tokens (subword units).\n",
    "sequence = \"Transformers are really cool, and not so difficult\"\n",
    "result = tokenizer(sequence)\n",
    "print(result)\n",
    "\n",
    "# Tokenizing the sequence into individual tokens (words/subword units) that the model understands.\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "\n",
    "# Converting tokens to their corresponding numerical IDs, as the model works with token IDs, not raw text.\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "# Decoding the token IDs back to human-readable text.\n",
    "# This demonstrates how the tokenized input is represented and can be decoded back into text.\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea4293-9659-4095-94cf-b3ea8fc56b69",
   "metadata": {},
   "source": [
    "#### 5) Implementing PyTorch into Natural Language Processing\n",
    "This code performs sentiment analysis using Hugging Face's Transformers and PyTorch. It first uses a pre-trained DistilBERT model in a sentiment analysis pipeline to classify a joke's sentiment. Then, it manually tokenizes the input text, converts it into tensors, and runs it through the model without computing gradients. The raw model outputs (logits) are processed with softmax to get probability scores, and the final sentiment label is determined using argmax(). This approach provides both a high-level and low-level understanding of sentiment classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "854a2fa1-b281-4230-a2ae-98a1a96fb530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9987579584121704}]\n",
      "{'input_ids': tensor([[  101,  2339,  2106,  1996, 19748,  3338,  2039,  2007,  1996,  7344,\n",
      "          1029,  2138,  1996,  3276,  2734,  2062,  2686,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6659, -3.0238]]), hidden_states=None, attentions=None)\n",
      "tensor([[0.9988, 0.0012]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries from Hugging Face Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch  # PyTorch library for tensor operations\n",
    "import torch.nn.functional as F  # Functional module for softmax and other operations\n",
    "\n",
    "# Define the pre-trained sentiment analysis model (DistilBERT fine-tuned on SST-2 dataset)\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load the pre-trained model for sequence classification (sentiment analysis)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer associated with the model to process text input\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the sentiment analysis pipeline using the model and tokenizer\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define a sample input sentence for sentiment analysis\n",
    "X_train = [\"Why did the astronaut break up with the alien? Because the relationship needed more space!\"]\n",
    "# Note: Even though this is a single sentence, it's placed in a list for batch processing compatibility.\n",
    "\n",
    "# Perform sentiment analysis using the pipeline\n",
    "result = classifier(X_train)\n",
    "print(result)  # Output contains sentiment label and confidence score\n",
    "\n",
    "# Tokenize the input text into a format suitable for the model\n",
    "batch = tokenizer(\n",
    "    X_train,           # Input text list\n",
    "    padding=True,      # Pads the input to the same length for batch processing\n",
    "    truncation=True,   # Truncates input if it exceeds max length (512 tokens)\n",
    "    max_length=512,    # Maximum token limit for DistilBERT\n",
    "    return_tensors=\"pt\"  # Return as PyTorch tensors for model compatibility\n",
    ")\n",
    "print(batch)  # Display tokenized batch\n",
    "\n",
    "# Perform inference without calculating gradients (for efficiency)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)  # Forward pass through the model\n",
    "    print(outputs)  # Print raw output logits\n",
    "\n",
    "    # Apply softmax to convert logits into probability scores for each class (POSITIVE or NEGATIVE)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)  # Print class probabilities\n",
    "\n",
    "    # Get the predicted class by taking the index of the highest probability\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)  # Output is either 0 (NEGATIVE) or 1 (POSITIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d1fa7-840b-4f33-87f6-dacd0b0e9108",
   "metadata": {},
   "source": [
    "#### 7) Saving and Loading a Model\n",
    "This process ensures that a trained model and tokenizer can be saved and reloaded, making deployment and reuse more efficient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5deef0-78c9-49ac-91e9-6cd034ba4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the model and tokenizer will be saved.\n",
    "save_directory = \"saved\"\n",
    "\n",
    "# Save the tokenizer to the specified directory.\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "# Save the trained model to the specified directory.\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Load the tokenizer from the saved directory.\n",
    "tok = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Load the model from the saved directory.\n",
    "mod = AutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
