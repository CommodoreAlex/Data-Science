{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba3d1e0-4d0d-4497-a650-1abedc7111e8",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: OpenAI Gym Practice  \n",
    "\n",
    "As a data science student, I am excited to share this short project where I focus on a first experience with reinforcement learning using OpenAI Gym. This project aims to provide hands-on practice with training reinforcement learning models using deep learning techniques.  \n",
    "\n",
    "## What is Reinforcement Learning  \n",
    "\n",
    "Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. Unlike supervised learning, RL does not rely on labeled data but instead learns through trial and error using feedback from its own actions. RL is widely used in robotics, game playing, finance, and autonomous systems.  \n",
    "\n",
    "## What is OpenAI Gym  \n",
    "\n",
    "OpenAI Gym is a toolkit developed by OpenAI for developing and comparing reinforcement learning algorithms. It provides a collection of pre-built environments, such as the popular CartPole and Atari games, that allow users to experiment with different RL techniques. Gym offers a simple API to interact with environments, making it a great starting point for RL research and development: [Gymnasium: OpenAI Gym's Successor](https://gymnasium.farama.org/)  \n",
    "\n",
    "## Learning Objectives  \n",
    "\n",
    "1. Create OpenAI Gym environments like CartPole  \n",
    "2. Build a Deep Learning model for Reinforcement Learning using TensorFlow and Keras  \n",
    "3. Train a Reinforcement Learning model using Deep Q-Network (DQN) based learning with Keras-RL\n",
    "\n",
    "## What is Cartpole\n",
    "\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart: [Gymnasium Website](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "## Notice\n",
    "\n",
    "This does require an environment set in Python3.8. \n",
    "\n",
    "## Lecture Resources for Reinforcement Learning  \n",
    "\n",
    "- [Reinforcement Learning from Scratch](https://www.youtube.com/watch?v=vXtfdGphr3c&list=PLhBFZf0L5I7oIFTNTclyvWRciXaVb76Yt&index=4)  \n",
    "- [An Introduction to Reinforcement Learning](https://www.youtube.com/watch?v=JgvyzIkgxF0&list=PLhBFZf0L5I7oIFTNTclyvWRciXaVb76Yt&index=5)  \n",
    "- [Reinforcement Learning: Machine Learning Meets Control Theory](https://www.youtube.com/watch?v=0MNVhXEX9to&list=PLhBFZf0L5I7oIFTNTclyvWRciXaVb76Yt&index=6)\n",
    "\n",
    "## Credit for Educational Objecitves of this Project\n",
    "- [Deep Reinforcement Learning Tutorial for Python in 20 Minutes](https://www.youtube.com/watch?v=cO5g5qLrLSo&list=PLhBFZf0L5I7oIFTNTclyvWRciXaVb76Yt&index=2)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c7d8d-edb1-4f77-9b2e-144229e65b96",
   "metadata": {},
   "source": [
    "#### 1) Install required libraries\n",
    "\n",
    "Note: We are installing 'gymnasium' instead of 'gym' but later importing it as 'gym' for simplicity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68315f1a-74b2-4453-a6c7-4e8acf51d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3.0\n",
    "!pip install gymnasium\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0122516-0c1e-4e22-80ab-be80551e2143",
   "metadata": {},
   "source": [
    "#### 2 Test Random Environment with OpenAI Gym\n",
    "\n",
    "In this code, we are training an agent to interact with the CartPole-v1 environment from OpenAI's Gym. The agent performs a series of episodes, where in each episode it starts in a random initial state. During each episode, the agent takes random actions (either 0 or 1, representing left or right movements of the cart) and receives rewards based on the environment's response. The environment is rendered after every action to provide visual feedback, allowing us to observe the agent's behavior. The agent continues acting until the episode ends, which happens when the pole falls down or the episode reaches its maximum time limit. The total score for each episode, which is the cumulative reward received, is tracked and printed after every episode to monitor the agent's performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765f19a-ad1f-487a-86bd-12588181052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import gymnasium as gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5532d-7e07-403d-95ee-4dbe2979066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Cartpole environment using Gym\n",
    "env = gym.make('CartPole-v1')  # Create an instance of the CartPole environment\n",
    "\n",
    "# Get the number of states (features) in the observation space\n",
    "states = env.observation_space.shape[0]  # The shape of the observation space (4 features for CartPole)\n",
    "\n",
    "# Get the number of possible actions (discrete action space)\n",
    "actions = env.action_space.n  # The number of possible actions (2: left or right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a2609-7650-4fed-a56c-f01826b25b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left or Right movements, this indicates there are 2 possible actions (1 or 0)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4a6bf-0134-4d11-a425-9fb2ac8bb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes for training\n",
    "episodes = 10\n",
    "\n",
    "# Loop through the number of episodes\n",
    "for episode in range(1, episodes + 1):  \n",
    "    # Reset the environment at the start of each episode\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    done = False  # This flag keeps track of whether the episode is finished\n",
    "    score = 0  # Initialize the score for this episode\n",
    "\n",
    "    # Keep interacting with the environment until the episode is done\n",
    "    while not done:\n",
    "        # Render the environment (useful for visual feedback)\n",
    "        env.render()\n",
    "        \n",
    "        # Randomly select an action (either 0 or 1 for CartPole)\n",
    "        action = random.choice([0, 1])\n",
    "        \n",
    "        # Perform the chosen action and observe the new state, reward, done flag, and additional info\n",
    "        state, reward, done, _, info = env.step(action)  # Ignore the 4th value by using '_'\n",
    "        \n",
    "        # Accumulate the score from the rewards\n",
    "        score += reward\n",
    "    \n",
    "    # Print the episode number and its score after the episode ends\n",
    "    print(f'Episode {episode}: Score = {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fa16f-6529-49df-bb12-3494e76d424a",
   "metadata": {},
   "source": [
    "#### 3) Create a Deep Learning Model with Keras\n",
    "\n",
    "In this section, we built a neural network model using TensorFlow and Keras to approximate the Q-function for a reinforcement learning task. We defined a function build_model that constructs a sequential neural network with two hidden layers, each containing 24 neurons with ReLU activation, and an output layer matching the number of possible actions with a linear activation. The model is designed to take the environment's state as input and predict the expected reward for each action. After building the model, we displayed its architecture using model.summary(), providing an overview of the layers and parameters in the network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f6ce2-d911-4c73-aeea-a6cb7d8f8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # NumPy is used for numerical operations and array handling\n",
    "\n",
    "# Importing components for creating and training a neural network\n",
    "from tensorflow.keras.models import Sequential  # Sequential model is used to create a linear stack of layers\n",
    "from tensorflow.keras.layers import Dense, Flatten  # Dense layer is a fully connected layer, Flatten flattens the input for the next layer\n",
    "from tensorflow.keras.optimizers import Adam  # Adam is an optimization algorithm used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ef579-7337-4c49-924f-8d443f4d8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a neural network model for reinforcement learning\n",
    "def build_model(states, actions):\n",
    "    # Initialize a Sequential model, which allows stacking layers linearly\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Flatten the input layer to reshape the state space into a 1D vector\n",
    "    # input_shape=(1, states) assumes the input is a 2D array (batch size, states) with the state space size defined by 'states'\n",
    "    model.add(Flatten(input_shape=(1, states)))  \n",
    "    \n",
    "    # Add a Dense hidden layer with 24 neurons and ReLU activation function\n",
    "    # ReLU (Rectified Linear Unit) introduces non-linearity to the model, allowing it to learn complex patterns\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    \n",
    "    # Add another Dense hidden layer with 24 neurons and ReLU activation function\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    \n",
    "    # Add the output layer with a number of neurons equal to the number of possible actions\n",
    "    # 'linear' activation function is used to output raw values (i.e., Q-values)\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    \n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7edb5-3015-4356-87e0-97c25d1bc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using the build_model function with the given number of states and actions\n",
    "model = build_model(states, actions)\n",
    "\n",
    "# Display the summary of the model architecture, including the layers, their output shapes, and the number of parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3872c9-9886-47e6-949e-9512aa884c1c",
   "metadata": {},
   "source": [
    "#### 4) Build Agent wtih Keras-RL\n",
    "\n",
    "This code sets up a reinforcement learning environment using a Deep Q-Network (DQN) agent. It imports necessary libraries like DQNAgent, BoltzmannQPolicy, and SequentialMemory. The DQN agent is built with a Boltzmann policy for action selection, which probabilistically favors actions with higher Q-values. The agent’s experiences are stored in a memory buffer, which is later used for training. After defining the agent, the code compiles it with the Adam optimizer and sets the learning rate. Finally, it trains the agent by fitting it to the environment for a specified number of steps, allowing the agent to learn from its interactions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb5c01-02dc-440e-ac45-c0292c91eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary components for reinforcement learning\n",
    "# DQNAgent: This is the Deep Q-Network agent which will be used to perform reinforcement learning using a neural network model.\n",
    "# BoltzmannQPolicy: This is the policy used for action selection, where the agent selects actions based on a probability distribution influenced by Q-values.\n",
    "# SequentialMemory: This is the memory used to store the agent's experiences during the training process, which can be replayed for training the model.\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Function to build the DQN agent\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()  # The action selection policy\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)  # Memory for storing experiences\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "# Build the agent\n",
    "dqn = build_agent(model, actions)\n",
    "\n",
    "# Compile the agent with the Adam optimizer and learning rate\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "# Fit the agent to the environment\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7b010-2095-4c6b-8ef7-79869db33ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of the trained DQN agent over 100 episodes\n",
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "\n",
    "# Calculate and print the average reward over all test episodes\n",
    "# 'scores.history' contains the rewards for each episode\n",
    "# 'episode_reward' is a list of rewards for each episode in the test\n",
    "# np.mean() computes the average of these rewards\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712eca1-d75d-4111-8540-2258f67a03f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will visaulize our model, where we can see the pole being balanced\n",
    "_ = dqn.test(env, nb_episodes=5, visualize=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f406641-1831-4f6a-b1a2-3f118bcff4b1",
   "metadata": {},
   "source": [
    "#### 5) Reloading Agent from Memory\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a2975-d4d9-49cf-a310-7ebd9fd35bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save the weights and reload them later, to test them out\n",
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e90f0-3e88-4070-83b9-7065cb558257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting\n",
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171ef39-345d-4a45-b2bc-0bc544c6fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilding the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f79475-bdf2-4214-8904-1cea2848d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload our weights into the model for testing\n",
    "dqn.load_weights('weights_filename.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c36a20-17a1-44fd-82c1-a764adee1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
